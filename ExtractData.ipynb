{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import xml.dom.minidom\n",
    "import pandas as pd\n",
    "from xml.etree import cElementTree as ET\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#import nltk.word_tokenize\n",
    "#import metapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"timeSeriesPrices.csv\")\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746234\n",
      "('05/03/00', 'BLAUSTEIN-Phoebe Evseroff. May 2, 2000. Beloved wife of the late Irving H. Blaustein, M.D. Devoted mother of Janet (Michael) Kublin and dear grandmother of Heather Ilene Kublin. Graveside services 11 AM Thursday, Beth El Cemetery, Paramus, N.J.')\n"
     ]
    }
   ],
   "source": [
    "docs = [] #(date, document_data)\n",
    "for month in os.listdir(\"./documents\"):\n",
    "    for day in os.listdir(\"./documents/\"+month):\n",
    "        for doc in os.listdir(\"./documents/\"+month+\"/\"+day):\n",
    "            tree = ET.parse(\"./documents/\"+month+\"/\"+day+\"/\"+doc)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            for page in root.findall('body'):\n",
    "                for page1 in page.findall('body.content'):\n",
    "                    for page2 in page1.findall('block'):\n",
    "                        for page3 in page2.findall('p'):\n",
    "                            if page3.text != None:\n",
    "                                docs.append((month+\"/\"+day+\"/00\", page3.text))                           \n",
    "\n",
    "            \n",
    "\n",
    "print(len(docs))\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23931\n",
      "In a speech pungent with tough talk about criminals, drug abusers and his Republican rival for president, Vice President Al Gore called today for a broad array of anticrime measures, among them mandatory drug testing of all prisoners and parolees.\n"
     ]
    }
   ],
   "source": [
    "D = []\n",
    "for doc in docs:\n",
    "    gore = 0\n",
    "    bush = 0\n",
    "    if \"Gore\" in doc[1]: \n",
    "        gore += 1\n",
    "        \n",
    "    if \"Bush\" in doc[1]:\n",
    "        bush += 1\n",
    "        \n",
    "    if gore > 0 or bush > 0:\n",
    "        D.append(doc)\n",
    "        \n",
    "print(len(D))\n",
    "print(D[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peterwasala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#for doc in D:\n",
    "    #print(doc[1])\n",
    "    #words = doc[1]\n",
    "    #words = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in doc[1]]\n",
    "    #words = [re.sub('\\s+', '', sent) for sent in doc[1]]\n",
    "    #words = [re.sub(\"\\'\", '', sent) for sent in doc[1]]\n",
    "    #print(words)\n",
    "    #i+=1\n",
    "    #if(i == 2):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    #print(sentences)\n",
    "    for sentence in sentences:\n",
    "        #print(sentence[1])\n",
    "        yield(gensim.utils.simple_preprocess(str(sentences[1]), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "\n",
    "data_words = list(sent_to_words(D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterwasala/opt/anaconda3/envs/py35/lib/python3.5/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['choosing', 'southern', 'state', 'as', 'the', 'backdrop', 'for', 'one', 'of', 'his', 'most', 'conservative', 'speeches', 'of', 'the', 'presidential', 'campaign', 'mr', 'gore', 'proposed', 'federal', 'spending', 'of', 'million', 'year', 'to', 'help', 'states', 'test', 'treat', 'and', 'counsel', 'prisoners', 'and', 'parolees', 'for', 'drug', 'use']\n"
     ]
    }
   ],
   "source": [
    "# Step 9\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# Step 10\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['choose', 'southern', 'state', 'backdrop', 'conservative', 'speech', 'presidential', 'campaign', 'mr', 'gore', 'propose', 'federal', 'spending', 'year', 'help', 'state', 'test', 'treat', 'counsel', 'prisoner', 'parole', 'drug']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "from spacy.cli import download\n",
    "#print(download('en'))\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('backdrop', 1),\n",
       "  ('campaign', 1),\n",
       "  ('choose', 1),\n",
       "  ('conservative', 1),\n",
       "  ('counsel', 1),\n",
       "  ('drug', 1),\n",
       "  ('federal', 1),\n",
       "  ('gore', 1),\n",
       "  ('help', 1),\n",
       "  ('mr', 1),\n",
       "  ('parole', 1),\n",
       "  ('presidential', 1),\n",
       "  ('prisoner', 1),\n",
       "  ('propose', 1),\n",
       "  ('southern', 1),\n",
       "  ('speech', 1),\n",
       "  ('spending', 1),\n",
       "  ('state', 2),\n",
       "  ('test', 1),\n",
       "  ('treat', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 11\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (1,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (2,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (3,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (4,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (5,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (6,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (7,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (8,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (9,\n",
      "  '0.048*\"presidential\" + 0.048*\"parole\" + 0.048*\"treat\" + 0.048*\"test\" + '\n",
      "  '0.048*\"state\" + 0.048*\"spending\" + 0.048*\"speech\" + 0.048*\"southern\" + '\n",
      "  '0.048*\"propose\" + 0.048*\"prisoner\"'),\n",
      " (10,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (11,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (12,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (13,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (14,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (15,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (16,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (17,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"'),\n",
      " (18,\n",
      "  '0.048*\"presidential\" + 0.048*\"parole\" + 0.048*\"treat\" + 0.048*\"test\" + '\n",
      "  '0.048*\"state\" + 0.048*\"spending\" + 0.048*\"speech\" + 0.048*\"southern\" + '\n",
      "  '0.048*\"propose\" + 0.048*\"prisoner\"'),\n",
      " (19,\n",
      "  '0.091*\"state\" + 0.045*\"presidential\" + 0.045*\"parole\" + 0.045*\"treat\" + '\n",
      "  '0.045*\"test\" + 0.045*\"spending\" + 0.045*\"speech\" + 0.045*\"southern\" + '\n",
      "  '0.045*\"propose\" + 0.045*\"prisoner\"')]\n"
     ]
    }
   ],
   "source": [
    "#Step 13\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -3.2752565433564347\n",
      "\n",
      "Coherence Score:  0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Step 14\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-2fae923079e4>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-2fae923079e4>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    for i, sig in enumerate(pos_impact)):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_prior(significance):\n",
    "    \"\"\" \n",
    "    Define a prior on the topic model parameters using significant terms and their impact values. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #Step 4a\n",
    "    pos_impact = [sig - .90 for sig in significance]\n",
    "    neg_impact = [-sig - .90 for sig in significance]\n",
    "    for i, sig in enumerate(pos_impact):\n",
    "        if sig < 0:\n",
    "            pos_impact[i] = 0\n",
    "            \n",
    "    for i, sig in enumerate(neg_impact):\n",
    "        if sig < 0:\n",
    "            neg_impact[i] = 0\n",
    "        \n",
    "    pos_impact = numpy.array(pos_impact)\n",
    "    neg_impact = numpy.array(neg_impact)\n",
    "    \n",
    "    \n",
    "    \n",
    "    prior = []\n",
    "    \n",
    "    for i in range(len(pos_impact)):\n",
    "        count_pos = np.count_nonzero(positive_sigs)\n",
    "        count_neg = np.count_nonzero(negative_sigs)\n",
    "        sum_impact = count_pos + count_neg\n",
    "        if sum_impact != 0:\n",
    "            prob = count_pos / sum_impact\n",
    "            #step 4b\n",
    "            if (prob > 0.9):\n",
    "                prior.append(pos_impact / np.sum(pos_impact))\n",
    "            elif (prob < 0.1):\n",
    "                prior.append(neg_impact / np.sum(neg_impact))\n",
    "            else:\n",
    "                prior.append(pos_impact / np.sum(pos_impact))\n",
    "                prior.append(neg_impact / np.sum(neg_impact))                \n",
    "    \n",
    "    if prior != None:\n",
    "        return numpy.array(prior)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
